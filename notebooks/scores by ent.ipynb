{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "import ujson as json\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import spacy\n",
    "from spacy.gold import GoldParse, tags_to_entities\n",
    "from spacy.scorer import Scorer, PRFScore\n",
    "from spacy.util import load_model_from_path\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/plays-small-batches/epoch-834\"\n",
    "file_path = \"../training/evaluation/fulltext-*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ('ACT', 'SCENE', 'OTHER-SEP', 'SPEAKER', 'M-SPEAKERS', 'HEAD-COMMENT', 'COMMENT', \n",
    "          'PAGE-NUMBER', 'FOOTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(nlp):\n",
    "    prefixes = deepcopy(nlp.Defaults.prefixes)\n",
    "    prefixes += (\"-\",)\n",
    "    \n",
    "    suffixes = deepcopy(nlp.Defaults.suffixes)\n",
    "    suffixes += (\"-\", \"(?<!\\.[A-Z])(?<=[A-Z])\\.\",)\n",
    "    \n",
    "    prefix_re = spacy.util.compile_prefix_regex(prefixes)\n",
    "    suffix_re = spacy.util.compile_suffix_regex(suffixes)\n",
    "\n",
    "    infix_re = spacy.util.compile_infix_regex(nlp.Defaults.infixes)\n",
    "    \n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=nlp.Defaults.token_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path):\n",
    "    with open(path, 'r', encoding=\"utf-8\") as file:\n",
    "        return json.loads(file.read())\n",
    "\n",
    "\n",
    "def load_files(path_glob):\n",
    "    files = glob(path_glob)\n",
    "    data = []\n",
    "    \n",
    "    for filepath in files:\n",
    "        data.append({\n",
    "            'path': filepath,\n",
    "            'json': load_file(filepath),\n",
    "        })\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = load_model_from_path(model_path)\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScorerEntity(Scorer):\n",
    "    def score_entity(self, tokens, gold, ent_label, verbose=False):\n",
    "        gold_ents = set(tags_to_entities([annot[-1]\n",
    "                        for annot in gold.orig_annot]))\n",
    "        \n",
    "        if '-' not in [token[-1] for token in gold.orig_annot]:\n",
    "            cand_ents = set()\n",
    "            for ent in tokens.ents:\n",
    "                if ent.label_ != ent_label:\n",
    "                    continue\n",
    "                \n",
    "                first = gold.cand_to_gold[ent.start]\n",
    "                last = gold.cand_to_gold[ent.end-1]\n",
    "                if first is None or last is None:\n",
    "                    self.ner.fp += 1\n",
    "                else:\n",
    "                    cand_ents.add((ent.label_, first, last))\n",
    "                    \n",
    "            self.ner.score_set(cand_ents, gold_ents)  \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_by_ent(text, annotations):\n",
    "    label_entities = {label: [] for label in LABELS}\n",
    "    scorers = {}\n",
    "    \n",
    "    doc = nlp.make_doc(text)\n",
    "    prediction = nlp(text)\n",
    "    \n",
    "    for annot in annotations['entities']:\n",
    "        label_entities[annot[2]].append(annot)\n",
    "           \n",
    "    for label in LABELS:\n",
    "        scorer = ScorerEntity()\n",
    "\n",
    "        gold = GoldParse(doc, entities=label_entities[label])\n",
    "\n",
    "        scorer.score_entity(prediction, gold, label)\n",
    "\n",
    "        scorers[label] = scorer\n",
    "        \n",
    "    return scorers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scorers(file_path, scorers):\n",
    "    print(file_path)\n",
    "    print(\n",
    "            \"{:>12}\".format(\"entity\"),\n",
    "            \"{:>8}\".format(\"ents_p\"),\n",
    "            \"{:>8}\".format(\"ents_r\"),\n",
    "            \"{:>8}\".format(\"ents_f\"),\n",
    "        )\n",
    "\n",
    "    for label, scorer in scorers.items():\n",
    "        print(\n",
    "            \"{:>12}\".format(label),\n",
    "            \"{:>8}\".format(round(scorer.scores['ents_p'], 2)),\n",
    "            \"{:>8}\".format(round(scorer.scores['ents_r'], 2)),\n",
    "            \"{:>8}\".format(round(scorer.scores['ents_f'], 2)),\n",
    "        )\n",
    "\n",
    "def print_global_scores(global_scores, all_scorers):\n",
    "    def percent(value):\n",
    "        return round(100 * value, 2)\n",
    "    \n",
    "\n",
    "    def max_of(label, name):\n",
    "        return round(max(map(lambda file_score: file_score['scorer'].scores[name], all_scorers[label])), 2)\n",
    "    \n",
    "    \n",
    "    def has_ent(scorer):\n",
    "        return (scorer.ner.tp + scorer.ner.fp + scorer.ner.fn) > 0\n",
    "    \n",
    "    def min_of(label, name):\n",
    "        scores = []\n",
    "        for file_score in all_scorers[label]:\n",
    "            if has_ent(file_score['scorer']) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                scores.append(file_score['scorer'].scores)\n",
    "                \n",
    "        return round(min(map(lambda scores: scores[name], scores)), 2)\n",
    "        \n",
    "    print(\n",
    "            \"{:<12}\".format(\"entity\"),\n",
    "        \n",
    "            \"{:>8}\".format(\"preci.\"),\n",
    "            \"{:>8}\".format(\"pre min\"),\n",
    "            \"{:>8}\".format(\"pre max\"),\n",
    "        \n",
    "            \"{:>8}\".format(\"recall\"),\n",
    "            \"{:>8}\".format(\"rec min\"),\n",
    "            \"{:>8}\".format(\"rec max\"),\n",
    "        \n",
    "            \"{:>8}\".format(\"fscore\"),\n",
    "            \"{:>8}\".format(\"fsc min\"),\n",
    "            \"{:>8}\".format(\"fsc max\"),\n",
    "        )\n",
    "\n",
    "    for label, score in global_scores.items():\n",
    "        print(\n",
    "            \"{:<12}\".format(label),\n",
    "            \n",
    "            \"{:>8}\".format(percent(score.precision)),\n",
    "            \"{:>8}\".format(min_of(label, \"ents_p\")),\n",
    "            \"{:>8}\".format(max_of(label, \"ents_p\")),\n",
    "            \n",
    "            \"{:>8}\".format(percent(score.recall)),\n",
    "            \"{:>8}\".format(min_of(label, \"ents_r\")),\n",
    "            \"{:>8}\".format(max_of(label, \"ents_r\")),\n",
    "            \n",
    "            \"{:>8}\".format(percent(score.fscore)),\n",
    "            \"{:>8}\".format(min_of(label, \"ents_f\")),\n",
    "            \"{:>8}\".format(max_of(label, \"ents_f\")),\n",
    "        )\n",
    "        \n",
    "        f_scores = {}\n",
    "        for file_score in all_scorers[label]:\n",
    "            if has_ent(file_score['scorer']) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                f_scores[file_score['path']] = round(file_score['scorer'].scores['ents_f'], 2)\n",
    "\n",
    "        min_path = min(f_scores, key=f_scores.get)\n",
    "        \n",
    "        if f_scores[min_path] < 100:\n",
    "            print(\"    Worst:\", min_path, f_scores[min_path], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = load_files(file_path)\n",
    "global_scores = {label: PRFScore() for label in LABELS}\n",
    "all_scorers   = {label: [] for label in LABELS}\n",
    "\n",
    "for file in file_data:\n",
    "    scorers = evaluate_by_ent(*file['json'])\n",
    "    \n",
    "    \n",
    "    for label, scorer in scorers.items():\n",
    "        all_scorers[label].append({ 'path': file['path'], 'scorer': scorer })\n",
    "        \n",
    "        global_scores[label].tp += scorer.ner.tp\n",
    "        global_scores[label].fp += scorer.ner.fp\n",
    "        global_scores[label].fn += scorer.ner.fn\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity         preci.  pre min  pre max   recall  rec min  rec max   fscore  fsc min  fsc max\n",
      "ACT             100.0    100.0    100.0    100.0    100.0    100.0    100.0    100.0    100.0\n",
      "SCENE           97.59      0.0    100.0    100.0      0.0    100.0    98.78      0.0    100.0\n",
      "    Worst: ../training/evaluation/fulltext-657.json 0.0 \n",
      "\n",
      "OTHER-SEP       44.83      0.0    100.0     50.0      0.0    100.0    47.27      0.0    100.0\n",
      "    Worst: ../training/evaluation/fulltext-657.json 0.0 \n",
      "\n",
      "SPEAKER         99.54    95.83    100.0    97.65    82.64    100.0    98.58    90.19    100.0\n",
      "    Worst: ../training/evaluation/fulltext-657.json 90.19 \n",
      "\n",
      "M-SPEAKERS      72.22      0.0    100.0    77.61      0.0    100.0    74.82      0.0    100.0\n",
      "    Worst: ../training/evaluation/fulltext-1018.json 0.0 \n",
      "\n",
      "HEAD-COMMENT    95.76    63.64    100.0    93.74    53.85    100.0    94.74    58.33    99.77\n",
      "    Worst: ../training/evaluation/fulltext-1018.json 58.33 \n",
      "\n",
      "COMMENT         67.95      0.0    100.0     73.4      0.0    100.0    70.57      0.0    100.0\n",
      "    Worst: ../training/evaluation/fulltext-986.json 0.0 \n",
      "\n",
      "PAGE-NUMBER     66.27      0.0    100.0    100.0      0.0    100.0    79.71      0.0    100.0\n",
      "    Worst: ../training/evaluation/fulltext-657.json 0.0 \n",
      "\n",
      "FOOTER           2.08      0.0    100.0    33.33      0.0    33.33     3.92      0.0     50.0\n",
      "    Worst: ../training/evaluation/fulltext-657.json 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_global_scores(global_scores, all_scorers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single path evaluation :\n",
    "text, annotations = load_file(file_path)\n",
    "scorers = evaluate_by_ent(text, annotations)\n",
    "print_scorers(file_path, scorers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
